<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>foosel.net Blog</title>
        <link>https://foosel.net/blog/</link>
        <description></description>
        <lastBuildDate>Mon, 07 Jun 2021 16:30:52 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <copyright>All rights reserved 2021, Gina HÃ¤uÃŸge</copyright>
        <item>
            <title><![CDATA[A debugging story]]></title>
            <link>https://foosel.net/blog/2021-05-09-a-debugging-story</link>
            <guid>2021-05-09-a-debugging-story</guid>
            <pubDate>Sun, 09 May 2021 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[
<p>About a week ago I got a new <a href="https://github.com/OctoPrint/OctoPrint/issues/4117">bug report</a> on <a href="https://octoprint.org">OctoPrint's</a> issue tracker:</p>
<blockquote>
  <p><strong>GCode Viewer Visualisation Problem</strong></p>
  <p><em>The problem</em></p>
  <p>
    The visualisation in GCode viewer ist not correct. The print is OK.
    See gcode file (zip) on Layer 43 to 47 and 49
  </p>
  <p>And screenshot</p>
</blockquote>
<p>
  You already saw the included screenshot, and it shows that there was a spike being visualized in the GCode Viewer that
  wasn't actually there. My first attempt at reproduction failed spectacularly -- the file looked exactly like
  it was supposed to. Then I noticed that the OP was using Google Chrome however (adding the detected user agent
  to the system information contained in OctoPrint's new System Info Bundles already paid off!) and tried with that
  instead of my usual Firefox, and lo and behold, I saw the issue.
</p>
<p>Scrolling a bit through the file revealed further defects, as also mentioned by the OP, e.g. this one:</p>
<p>
  <img src="./issue_4117_2.png" alt="Another defect, this time a whole part of the outline is being misplaced">
</p>
<p>
  At this point it was clear that this was a Chrome-only issue. But was it a bug in OctoPrint or possibly a browser
  bug? More information for that was needed but not readily available, and the file was also too big to quickly
  gleam anything from the GCode itself that could possibly help to narrow down on the problem.
</p>
<p>
  So the first step was to create a minimal GCode file that showed the same error. For this I took a look at
  the reported layer height in the viewer on the layer a defect was visible and then narrowed down on the affected lines
  by using the horizontal command sliders to further limit the view. That way I quickly found that these were the
  problematic lines:
</p>
<pre class="language-gcode"><code class="language-gcode">G1 X173.595 Y103.9 E247.16716
G3 X173.600 Y126.097 I-105613.507 J39.645 E248.20080
G1 X169.552 Y126.098 E248.38933
</code></pre>
<p>
  More specifically, the error was caused by the contained <a href="https://reprap.org/wiki/G-code#G2_.26_G3:_Controlled_Arc_Move"><code>G3</code> command</a>,
  which instructs the printer to move in a counter clockwise arc
  from its current position to the given X and Y coordinates, with the center of said arc offset by the given I and J
  parameters. In the case of these lines, that meant to move in an arc from <code>(173.595, 103.0)</code> to <code>(173.600, 126.097)</code>
  with the arc's center at <code>x = 173.595 + (-105613.507) = -105439.912</code> and <code>y = 103.9 + 39.645 = 143.54500000000002</code>.
  Or in other words, a rather short arc with an enormous radius of over 105m that was more a straight line than
  an arc really. And that line was being drawn too long, causing the weird spike in the rendition.
</p>
<p>
  In order to understand how that could happen however we need to take a look at how the GCode viewer is implemented and how
  arcs work in that implementation. At its core, the GCode viewer is an HTML5 2D canvas on which the path described in
  a GCode file gets drawn. Commands like <code>G0</code> and <code>G1</code> that describe straight lines are drawn using <a href="https://developer.mozilla.org/en-US/docs/Web/API/CanvasRenderingContext2D/lineTo"><code>lineTo</code></a>,
  arcs as described by <code>G2</code> and <code>G3</code> are drawn using <a href="https://developer.mozilla.org/en-US/docs/Web/API/CanvasRenderingContext2D/arc"><code>arc</code></a>.
</p>
<p>
  <code>arc</code> takes six parameters: the <code>x</code> and <code>y</code> coordinate of the center of the arc, the radius <code>r</code>, the <code>startAngle</code> determining from which angle to start
  drawing the arc and the <code>endAngle</code> until which to draw the arc, and a flag that's <code>true</code> for counter clockwise and <code>false</code> or empty for clockwise.
  It is obvious this doesn't directly translate to the data contained in the GCode itself, where we rather have three points defining the arc -- a start
  point, and end point, and the arc's center. So we need to translate this into the data required by the <code>arc</code> method. Using some trigonometry,
  that is fairly straightforward:
</p>
<pre class="language-js"><code class="language-js"><span class="token comment">// given: G2/G3 X&#x3C;endX> Y&#x3C;endY> I&#x3C;i> J&#x3C;j>, &#x3C;startX>, &#x3C;startY></span>
arcX <span class="token operator">=</span> startX <span class="token operator">+</span> i<span class="token punctuation">;</span>
arcY <span class="token operator">=</span> startY <span class="token operator">+</span> j<span class="token punctuation">;</span>
r <span class="token operator">=</span> Math<span class="token punctuation">.</span><span class="token function">sqrt</span><span class="token punctuation">(</span>i <span class="token operator">*</span> i <span class="token operator">+</span> j <span class="token operator">*</span> j<span class="token punctuation">)</span><span class="token punctuation">;</span>
startAngle <span class="token operator">=</span> Math<span class="token punctuation">.</span><span class="token function">atan2</span><span class="token punctuation">(</span>startY <span class="token operator">-</span> arcY<span class="token punctuation">,</span> startX <span class="token operator">-</span> arcX<span class="token punctuation">)</span><span class="token punctuation">;</span>
endAngle <span class="token operator">=</span> Math<span class="token punctuation">.</span><span class="token function">atan2</span><span class="token punctuation">(</span>endY <span class="token operator">-</span> arcY<span class="token punctuation">,</span> endX <span class="token operator">-</span> arcX<span class="token punctuation">)</span><span class="token punctuation">;</span>
ccw <span class="token operator">=</span> <span class="token punctuation">(</span>command <span class="token operator">===</span> <span class="token string">"G3"</span><span class="token punctuation">)</span>
</code></pre>
<p>
  <img src="./drawing.png" alt="The parameters and their relation">
</p>
<p>
  My first guess was that the result of this conversion was somehow different between Firefox and Chrome, and so I modified the GCode viewer to log
  the calculated values and then compared the two outcomes. The values were completely identical between both browsers, so what was being fed
  into the canvas <code>arc</code> command was identical and yet produces different results. Why?
</p>
<p>
  My next approach was to add some more visual debug output to the viewer itself. I modified it such that the arc parameters as calculated would
  actually be drawn on the canvas as well, in form of a geometrical pizza slice showing the arc's center, its "legs" and its rim. And this is where
  I saw a difference in the rendered output. Where in Firefox the arc's rim and its legs met perfectly:
</p>
<p>
  <img src="./arc_ff.png" alt="The arc in Firefox is rendered correctly">
</p>
<p>in Chrome the rim overshot:</p>
<p>
  <img src="./arc_chrome.png" alt="The same arc in Chrome is rendered wrong">
</p>
<p>So while the calculated parameters were correct and in both cases provided to the <code>arc</code> method just the same, Chrome was rendering the wrong segment length!</p>
<p>I suspected a rounding error and thus started searching for matching reports from other people. I couldn't find a specific bug report, but I came across a post on Stack Overflow that sounded mightily familiar: <a href="https://stackoverflow.com/questions/8603656/html5-canvas-arcs-not-rendering-correctly-in-google-chrome">HTML5 canvas arcs not rendering correctly in Google Chrome</a>, from 2011. A ten year old post... could it be?</p>
<p>Honestly, I still do not know if this indeed described the same issue or not, or if there's a Chrome ticket describing this behaviour -- I'll continue to look, but first and foremost I was focused on fixing this problem in OctoPrint's GCode viewer. The Stack Overflow post provided a code snippet that reimplements <code>arc</code> utilizing bezier curves, and so I gave this a try. Long story short, OctoPrint's GCode Viewer as part of version 1.7.0+ will ship with a Chrome-only <code>arc</code> replacement that will be enabled by default, but can also be disabled in real time, with great effect:</p>
<p>
  <img src="./arc_fix.gif" alt="Enabling and disabling the arc workaround makes the defects disappear and reappear">
</p>
<p>And the moral of the story: It rarely is a browser bug. But sometimes, all signs say it indeed <em>is</em> and a workaround is the easiest solution.</p>
]]></content:encoded>
            <enclosure url="https://foosel.net/assets/content/blog/2021-05-09-a-debugging-story/issue_4117.png" length="0" type="image/png"/>
        </item>
        <item>
            <title><![CDATA[Homelab uplink monitoring]]></title>
            <link>https://foosel.net/blog/2021-03-28-homelab-uplink-monitoring</link>
            <guid>2021-03-28-homelab-uplink-monitoring</guid>
            <pubDate>Sun, 28 Mar 2021 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[
<p>For a bit more than two years now I've been closely monitoring my network uplink. In the past I had a ton of issues with up- or download speeds not being what I paid for, packet loss issues and outright full blown outages. In order to put myself into a better position when reaching out to the ISP's support hotline I figured it would be good to be able to proof not only the existence of these issues but to also be able to determine the exact times they happened at and also to verify and show that in fact it was only external connections that were suffering and it was not an issue with my own internal network. Given that I don't trust the cable modem/router they force on me to be my edge router and instead have my own Unifi gear set up behind it (considering anything not exclusively under my control to be part of the hostile public internet) this otherwise will usually lead to endless attempts to blame my LAN when in fact the issue lies outside of my reach.</p>
<p>I already had an <a href="https://www.influxdata.com/">InfluxDB</a> and <a href="https://grafana.com/">Grafana</a> setup running anyhow for my <a href="https://home-assistant.io/">Home Assistant instance</a> to dump values from my home climate sensors into, so it was a logical next step to simply add some additional sensors to the mix.</p>
<h2>Throughput</h2>
<p>I currently run a speed test of the network throughput every 20min and log the results via MQTT into InfluxDB. I had to find out that neither the speed test integration in Home Assistant nor the official speedtest-cli tool were performing reliably enough for this -- I was constantly getting dips in measured throughput and thus alerts, even when everything was completely fine with my uplink.</p>
<p>I solved this by turning to <a href="https://github.com/nelsonjchen/speedtest-rs">speedtest-rs</a> and a small shell script that parses the output and pushes it into MQTT to Home Assistant, which then processes it further for some visualization right on my dashboard but also forwards it further into InfluxDB. You can find the <code>Dockerfile</code> and the script plus some further info <a href="https://gist.github.com/foosel/ef98a5774d1a495ab3781eba8a157fee">in this gist</a>.</p>
<p>In Grafana I then use this data to provide me with some single stat panels for the current downstream, upstream and ping values as well as the averages over the selected time range:</p>
<p>
  <img src="./currentspeed.png" alt="Some single stat panels show current and average down- and upstream speed and measured ping">
</p>
<p>Additionally, I also plot the down- and upstream speed in a timeline, together with the current bandwidth consumption as extracted by Home Assistant from my ISP's cable modem/router (thanks to the <a href="https://www.home-assistant.io/integrations/fritzbox_netmonitor/">Fritzbox NetMonitor integration</a>). Together, this gives me a good picture of whether there is actually an issue when I see a dip in the measured values, or if it's just too high bandwidth utilization:</p>
<p>
  <img src="./bandwidth.png" alt="A graph showing measured up- and downstream speed vs consumed up- and downstream bandwidth utilization">
</p>
<p>You can see in these screenshots that I recently upgraded my plan with my ISP -- from 200/20 to 500/50 MBit. The problem: The speedtest run by my monitoring setup doesn't hit the 500 mark, whereas running a manual test on speedtest.net works just fine. Looking at the <code>speedtest-rs</code> README it becomes apparent that this is a known issue with the legacy (open) Speedtest.net API:</p>
<blockquote>
  <p>This tool currently only supports <a href="http://www.ookla.com/support/a84541858">HTTP Legacy Fallback</a> for testing.</p>
  <p>High bandwidth connections higher than ~200Mbps may return incorrect results!</p>
  <p>The testing operations are different from socket versions of tools connecting to speedtest.net infrastructure. In the many FOSS Go versions, tests are done to find an amount of data that can run for a default of 3 seconds over some TCP connection. In particular, <code>speedtest-cli</code> and <code>speedtest-rs</code> tests with what Ookla calls the <a href="http://www.ookla.com/support/a84541858">"HTTP Legacy Fallback"</a> for hosts that cannot establish a direct TCP connection.</p>
</blockquote>
<p>I fear I might have to look into reimplementing the current speedtest-to-mqtt setup with another container utilizing the official (and sadly proprietary) Speedtest CLI tool to mitigate this issue. Thankfully, it should be quite easy to build a drop-in replacement thanks to the modularization in effect.</p>
<p><em>Update from March 30th 2021</em> I've now done that and <a href="https://gist.github.com/foosel/70ecbeade55cc852dbc0a4f7c4040adc">here's an updated gist</a> that works identically to the <code>speedtest-rs</code> approach, but instead utilizes <a href="https://www.speedtest.net/apps/cli">Ookla's official command line tool</a>. The results are stable numbers that reflect the expected bandwidth and also match the web based test results.</p>
<p><em>Update from March 31st, 2021</em> I wasn't too happy with running a proprietary tool for my speed testing, went looking for an OSS alternative, came across <a href="https://librespeed.org/">librespeed</a> and therefore have now <a href="https://gist.github.com/foosel/f7d9a08c0445454ab90d6c4974a9e316">replicated the setup again using that</a>. You might want to experiment a bit to find a server close to you and define that via <code>--server &#x3C;id></code>, the auto discovery appears to be a bit wonky. Or just use your own server list via <code>--server-json</code> or <code>--local-json</code>.</p>
<h2>Latency and packet loss</h2>
<p>In addition to the available up- and downstream speeds, I constantly monitor latency and packet loss to a selected number of hosts both external and internal to my network as well. For this I ping some public DNS servers (Google, Cloudflare and Quadnine) and some of my own vservers for the remote side, and the ISP's Fritzbox, my managed network gear and internal servers for the LAN side. I used to do this via <a href="https://oss.oetiker.ch/smokeping/">Smokeping</a>, but when I set up my InfluxDB/Grafana stack I wanted to find a solution to have everything together in one place.</p>
<p>Thankfully I almost immediately found <a href="https://hveem.no/visualizing-latency-variance-with-grafana">this post by Tor Hveem</a> who solved this with a little custom Go tool to run <code>fping</code> against a number of configurable hosts and push the results right into InfluxDB. This was exactly what I wanted and thus I replicated the outlined setup, albeit with a slightly different color scheme.</p>
<p>I use a <a href="https://github.com/nickvanw/infping">modified version of Tor's <code>infping</code> tool maintained by Nick Van Wiggeren</a> and run that in a Docker container on my NAS. You can find everything needed to run this on your own <a href="https://gist.github.com/foosel/46804306d510d79f14117f95ed64b877">in this gist</a>.</p>
<p>As a result I get ping output for all hosts every 60 sec with times and packet loss information pushed right into InfluxDB. This is easily queried by Grafana and looks quite nice when visualized:</p>
<p>
  <img src="./smokeping.png" alt="A graph showing min, avg and max latency and packet loss data for 8.8.8.8">
</p>
<p>And on my network dashboard, I plot only the <code>avg</code> values across all hosts and a mean <code>loss</code> value into one single graph each for external and internal hosts:</p>
<p>
  <img src="./latency.png" alt="A graph showing avg latency and packet loss data for all remote hosts">
</p>
<p>This allows me a good overview of the current state of uplink and internal network at one glance.</p>
<h2>Alerts</h2>
<p>Since just graphs won't give me an immediate heads-up when something goes wrong, I have a bunch of alerts set up in Grafana:</p>
<ul>
  <li>Measured download speed falls beneath 250MBit for more than one hour</li>
  <li>Measured upload speed falls beneath 35MBit for more than one hour</li>
  <li>Mean packet loss across all external hosts rises above 25% for more than ten minutes</li>
</ul>
<p>All of those trigger a notification to a private Discord server (via Grafana's own notification mechanism). In theory this notification should even include a screenshot of the panel for which the alert was triggered for, but I'm having some problems with that still that I need to investigate.</p>
<p>
  <img src="./discord.png" alt="An example alert and alert clearance notification in Discord">
</p>
<p>This notification channel has an obvious problem: When the uplink goes out completely, I won't get the notification if my phone is in my LAN. I really need to add a local alert as well at some point ðŸ˜…</p>
<p>Still, it usually will give me a heads-up in time for me to reach out to my ISP on short notice and request they start troubleshooting.</p>
<h2>Conclusion</h2>
<p>This monitoring setup has proven valuable in debugging network performance issues and also getting an early heads-up about current ISP issues. I have successfully used screenshots for proving ongoing issues to my ISP, and also sped up the one or other troubleshooting session when there was in fact an issue with my LAN. In my book, that makes it absolutely worth the time it took me to set this up and maintain it. And: it kinda looks cool ðŸ˜Ž</p>
<p>If you want to give this a go yourself, this might be of interest to you:</p>
<ul>
  <li><a href="https://gist.github.com/foosel/f7d9a08c0445454ab90d6c4974a9e316">Dockerfile, compose and instructions for speedtest container</a>
    <ul>
      <li><a href="https://gist.github.com/foosel/70ecbeade55cc852dbc0a4f7c4040adc">Ookla speedtest based version</a></li>
      <li><a href="https://gist.github.com/foosel/ef98a5774d1a495ab3781eba8a157fee">speedtest-rs based version</a></li>
    </ul>
  </li>
  <li><a href="https://gist.github.com/foosel/46804306d510d79f14117f95ed64b877">Dockerfile, compose and instructions for infping container</a></li>
  <li><a href="https://gist.github.com/foosel/ec0b6355d1d0c3ab65ee4df79d795a73">Panel JSON for the mentioned visualizations</a></li>
</ul>
]]></content:encoded>
            <enclosure url="https://foosel.net/assets/content/blog/2021-03-28-homelab-uplink-monitoring/title.png" length="0" type="image/png"/>
        </item>
        <item>
            <title><![CDATA[On wrong assumptions]]></title>
            <link>https://foosel.net/blog/2021-03-19-on-wrong-assumptions</link>
            <guid>2021-03-19-on-wrong-assumptions</guid>
            <pubDate>Fri, 19 Mar 2021 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[
<p><em>The original version of this post was published as a <a href="https://twitter.com/foosel/status/1242121324438355974">Twitter thread on March 23rd 2020</a>. I figured I should give it a more permanent home here since IMHO it was a quite fun story.</em></p>
<p>Since everyone can use some entertainment right now, how about a battle story on how a year ago I spent almost two weeks trying to wrap my head around a really weird issue of a lagging GCODE viewer and overall print progress reporting in <a href="https://octoprint.org">OctoPrint</a> and finally figuring it out?</p>
<p>Our story begins around the release of 1.4.0, when <a href="https://community.octoprint.org/t/curious-issue-with-print-progress/16304">a new topic on the community forum</a> showed up:</p>
<blockquote>
  <h3>Curious issue with print progress</h3>
  <p>The print progress figures on my Octopi setup are lagging behind the actual print. [...] Nothing is broken - anything I throw at it (an Ender 3) prints fine but as a print progresses, the percentage complete, current layer, and sync'd gcode viewer gradually lag behind what is actually being printed. For example, on a print with 400 layers, as the last layer is printed the reported progress and current layer is around 96% and 385 respectively. If I do a quick calculation of the displayed Printed/Total file size figures it works out at 96% but what it has actually printed is over 99%. When the print finishes the numbers jump to 100% and 400 and everything is fine.</p>
  <p>[...]</p>
</blockquote>
<p>This was indeed a very curious issue, since due to the nature of the communication with the printer and buffering in the firmware the progress is usually rather slightly <em>ahead</em> than behind. Some quick testing on my end showed no reproduction, however more and more people chimed in with the same observation.</p>
<p>I was stumped.</p>
<p>My first approach was to collect information from those affected by it. Printer model, firmware version, installed plugins, used slicer and so on. It soon turned out that all affected installations were using Ultimaker Cura as the slicer.</p>
<p>A quick test by the OP with a different slicer confirmed that it indeed just occurred with GCODE sliced by Cura for him, same file in another slicer had everything work as designed. However, comparing the GCODE revealed no immediate differences that would explain this, and what actually is <em>in</em> the file also doesn't really play into progress tracking. My own experiments with Cura failed to reproduce.</p>
<p>Convinced that the issue must be some sort of delay between the backend and the frontend -- maybe due to network issues? -- I whipped up a plugin (since deleted) to log progress on both ends to a log which could then be shared and analysed. The first results came in an guess what? I had barked up the wrong tree, the reported progress was identical. So back to square one.</p>
<p>I still couldn't reproduce it on my end and was starting to get really angry at this issue ðŸ˜… I finally threw a copy of some GCODE files now shared by the reporter of the issue on my own printer and <em>finally</em> I could reproduce. Which doesn't mean I had any idea WTF was going on though.</p>
<p>After many test prints, head scratching and going through the files with a comb I finally noticed something. The files with the issue had <code>CRLF</code> (or <code>\r\n</code>) line endings. Those without (including my own sliced files) had just <code>LF</code> (or <code>\n</code>) line endings.</p>
<p>So that made me go ðŸ¤¨ Some cursing and breakpoint setting later I had proof that the reported progress in backend and frontend was flawed to begin with. I could see that a line was being reported with a file position that it actually was not located at in the file, and which instead belonged to a couple lines earlier. Which meant my positions were reported wrong right at the source -- with a lag. And then it suddenly hit me.</p>
<p>But before I can tell you what was happening I need to give you some background on how OctoPrint reads GCODE files it's printing in order to understand what was going on. Printed files are read line by line because that is how they are sent to the printer. For that OctoPrint uses the <a href="https://docs.python.org/3/library/io.html?highlight=readline#io.IOBase.readline"><code>readline</code></a> method of the file stream. And that works by reading chunks of data from the file until a line separator is found, returning everything read up to this separator and saving the rest for the next line to be read. That means the file will have to be read further than what is returned. And that means that the position in the open file as reported by <a href="https://docs.python.org/3/library/io.html?highlight=readline#io.IOBase.tell"><code>tell</code></a> on the file stream will always be slightly ahead. For progress reporting in OctoPrint however I need to know the exact byte position of each line in the file. So what I do instead of relying on the internal and slightly ahead file position is that I increase my own position indicator by the length of the line read from the file. And this is where my problem was located.</p>
<p>It turns out that for some reason I wasn't getting the lines back from <code>readline</code> with the original line endings attached. Instead I always got <code>LF</code>, even for files with <code>CRLF</code>. And that means I was counting one byte short for every single line in <code>CRLF</code> terminated files. One byte short per line doesn't sound like much, but that adds up through a file with several hundred thousands of lines, to a point where progress reporting will be off by whole layers the further in the print and thus the file you are.</p>
<p>But what was the reason for this popping up in 1.4.0? I hadn't modified the code in question at all. It had been the same since 2016 actually. Well, it turns out that a tiny change during the Python 3 compatibility migration done to a helper function I used in that code had interesting side effects: switching from <a href="https://docs.python.org/3/library/codecs.html#codecs.open"><code>codecs.open</code></a> to <a href="https://docs.python.org/3/library/io.html#io.open"><code>io.open</code></a>.</p>
<p>It turns out that <code>io.open</code> (and thus Python 3's built-in <code>open</code>) by default will open text files in "universal newlines mode" (see <a href="https://www.python.org/dev/peps/pep-0278/">PEP278</a>), meaning it will happily parse every common line ending, but convert it to <code>LF</code> before returning. Which caused my off-by-one issue in files with <code>CRLF</code>.</p>
<p>And the fix? <a href="https://github.com/foosel/OctoPrint/commit/27bbab9582eb3a1a9fca8f2b203e88b1682fcdc5">Setting <code>newline=""</code> on the open call</a>:</p>
<pre class="language-diff"><code class="language-diff">diff --git a/src/octoprint/util/comm.py b/src/octoprint/util/comm.py
index 67191a7af..a6dfc1e24 100644
--- a/src/octoprint/util/comm.py
+++ b/src/octoprint/util/comm.py
@@ -4078,7 +4078,7 @@ def start(self):
 		"""
 		PrintingFileInformation.start(self)
 		with self._handle_mutex:
-			self._handle = bom_aware_open(self._filename, encoding="utf-8", errors="replace")
+			self._handle = bom_aware_open(self._filename, encoding="utf-8", errors="replace", newline="")
 			self._pos = self._handle.tell()
 			if self._handle.encoding.endswith("-sig"):
 				# Apparently we found an utf-8 bom in the file.
</code></pre>
<p>The moral of the story? Don't trust your file position calculations. I could have saved myself a lot of time on debugging this if I had just looked there <em>first</em> instead of assuming this code to be fine ðŸ˜…</p>
<p>In the end, even a year later, I still have no idea why Cura produced <code>CRLF</code> code for some and <code>LF</code> for me, but I also never really looked hard. A UNIX vs Windows issue can be ruled out here since the affected parties and me were all using Windows. It made me learn something about <code>io.open</code> and was a valuable lesson on wrong assumptions however!</p>
]]></content:encoded>
            <enclosure url="https://foosel.net/assets/content/blog/2021-03-19-on-wrong-assumptions/screenshot.jpg" length="0" type="image/jpg"/>
        </item>
        <item>
            <title><![CDATA[My workplace setup]]></title>
            <link>https://foosel.net/blog/2021-03-13-my-workplace</link>
            <guid>2021-03-13-my-workplace</guid>
            <pubDate>Sat, 13 Mar 2021 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[
<p>I've been working full time from my home office since mid-2014 now. At the time of writing this post this is nearing 7 years. Naturally, considering how much time I spend there, I've also spent a lot of thought and money on making sure my workplace helps to keep the usual side effects of the mostly sedentary lifestyle of a developer at bay.</p>
<p>Over the years I've had some run ins with RSI and backpain. My first wrist issues developed more than 10 years ago. Pain in my lower back beyond "all fine again after a good night's sleep" started in February of 2014. Both have been repeating visitors since then. You can probably imagine that that has led to a lot of research and experimentation to see what works and what doesn't for me. So, here's a summary of my findings as of March 2021. Quick disclaimer though, this is what has proven to work for <strong>me</strong>, that doesn't mean it will work for you, if in doubt please consult a professional. Also, I do link to some products here -- consider those references to give you more details on my setup, not official endorsement or anything like that.</p>
<h2>Keyboard and mouse</h2>
<p>For my wrists, <strong>ergonomic keyboards</strong> have proven to be crucial in combating the dreaded pain and numbness. I started with a <a href="https://www.microsoft.com/en-us/p/natural-ergonomic-keyboard-4000/">Microsoft Natural Ergonomic Keyboard 4000</a> (what a name...), switched over to an <a href="https://www.microsoft.com/en-us/p/microsoft-sculpt-ergonomic-desktop/">Microsoft Ergo Sculpt</a>, had a quick detour over a regularly shaped <a href="https://www.duckychannel.com.tw/en/Ducky-One-RGB-TKL">Ducky One TKL</a> to get my feet wet in mechanical keyboards and these days have arrived at the 1st gen <a href="https://ultimatehackingkeyboard.com/">Ultimate Hacking Keyboard</a>. It's a mechanical split keybord, sized at 60% (which means it has less keys than your common 101-key keyboard, only 60% of them to be precise, and compensates for that with the use of layers reached through modified keys) and fully programmable. I'm still optimizing the macros I have configured on it. I got it with red switches (linear and non clicky, I can't stand keyboards I can't use while holding a conversation ;)) and put a Git-themed keyset on it which I absolutely adore. And it's finally made me switch to US ANSI layout, which indeed is way better suited for coding than ISO DE. The UHK also supports some additional modules, and I have a trackpoint and an additional thumb keycluster on order once they finally release.</p>
<p>
  <img src="./keyboard.jpg" alt="Closeup of my Ultimate Hacking Keyboard">
</p>
<p>No matter how good you can memorize keyboard shortcuts (or how well the mouse layer of the UHK works), you still also need a <strong>mouse</strong>. In my case that's been gaming mice exclusively for 15+ years now, with a ton of turnover due to wear out or quality issues. Currently I'm sporting a <a href="https://steelseries.com/gaming-mice/rival-310">Steelseries Rival 310</a> after my last mouse, a Roccat Kone XTD, developed a flaky mouse wheel I couldn't fix, even though I tried my best. Apparently a design flaw. The Steelseries has so far worked nicely, but I've only had it for less than six months at the time of writing this. It's an asymmetric mouse tailored for use with the right hand. I can reach the side buttons easily and it isn't too heavy or too light either.</p>
<p>I also have a <strong>trackball</strong> though, dedicated to be used with my <em>left</em> hand. Whenever I notice my right wrist acting up, I switch to exclusive trackball use for a while, and that has managed to still avert Bad Things a number of times now. If you find yourself regularly suffering from RSI issues on your mouse hand, I can really recommend to mix things up with a trackball on your left. Personally I got a <a href="https://www.kensington.com/p/products/electronic-control-solutions/trackball-products/slimblade-trackball/">Kensington Slimblade</a>. It's symmetrical and the buttons are easily remapped to fit a left hand operation. And the huge ball can also be used as a scrollwheel and is actually also a great fidget toy to have on the desk at all times ;)</p>
<h2>Desk and chair</h2>
<p>In my opinion, there are two pieces of furniture you should never cheap out on: your <strong>office chair</strong> and your bed. I bought my <a href="https://www.sedus.com/en/products/chairs/netwin">sedus netwin</a> office chair right after finishing university in 2007 and apart from having gotten a bit more dirty here and there it's as good as new. I initially got it with a set of arm rests, but quickly figured out that those were actually detrimental to my posture and made me pull my shoulders up, leading to tension related pain. So I removed them. I've also gotten it a bit more pronounced lumbar support through the aid of <a href="https://www.amazon.de/gp/product/B07PB7G3QJ/">an add-on</a>, the likes of which you can order online for less than 10â‚¬ a piece. If I were to buy a new chair today, I'd probably get one with a head rest to keep me a bit more from slouching during long debugging or gaming sessions, but all in all I'm still completely happy with it, and the mesh back has proven to make sitting on it on hot summer days more bearable.</p>
<p>
  <img src="./chair.jpg" alt="My office chair, with a lumbar support addon">
</p>
<p>I've now had an <strong>electric standing desk</strong> since January 2016. I'm currently trying to get into the habit of daily use of the standing mode again and so far it's looking good. I got the <a href="https://www.ikea.com/de/de/p/bekant-schreibtisch-sitz-steh-weiss-s69022537/">IKEA Bekant</a> desk, and have since modified it to use the alternative <a href="https://github.com/gcormier/megadesk">Megadesk</a> controller to give it position memory slots (and hopefully also to hook it up into my home automation system long term). I can't stress enough what a difference it can make to just stand for a while during your work day. Exclusively standing should definitely also be avoided (I've had to do this recently for a couple of days because my back would <em>not</em> take sitting for an answer anymore, and it did a bit of a number on my ankles and knees), but regularly switching between sitting and standing is a great way not only to keep your posture intact but also to just get back your concentration. I currently work sitting until my lunch break, then switch to a couple hours of standing before either returning to sitting or calling it a day.</p>
<p>Speaking of sitting and standing -- I also recently acquired a bunch of accessories to make that more dynamic. While standing, I now regularly plant my feet on a <strong>balance board</strong> (a <a href="https://www.my-gymba.de/en">Gymba</a> one in my case). It allows me to move while standing, to vary my stance more easily and frankly, it's also a ton of fun to seesaw back and forth while working. I use it with shoes - you can also use it barefoot/in socks, but frankly that was a bit to tough a surface or my feet. Your mileage may vary of course.</p>
<p>I also got myself a <strong>wobble stool</strong> from <a href="https://www.flexispot.com/height-adjustable-wobble-stool-bh1b">Flexispot</a>. Imagine a stool, but instead of being stable it has a rounded base that makes it constantly wobble around. You cannot easily slouch on that, you'll fall over. Mine can be height adjusted from 61 to 82cm, so I use it both while standing as a small break, but also while sitting. I'm still getting used to it and am experimenting with heights and best way to sit on it, but it's so far been a great addition and doesn't take up much space (a serious plus in my limited office space).</p>
<h2>Monitor mounts</h2>
<p>Last but not least, I've got my two main monitors mounted on a <strong>dual monitor mount</strong>, in my case a <a href="http://www.puremounts.de/pm-office-dm-23d.html">gas lift one from PureMounts</a> (the small third monitor is mounted to the second one with a self designed printed mounting solution). The stands usually included with monitors tend to not offer enough flexibility to truly dial in the position of the screen in my experience, and this also managed to free up a <em>ton</em> of desktop real estate that I can now utilize. In my case, a wallmount is not an option due to the standing desk situation, so I instead went for a desk mount. A gas lift is not the most stable option in my experience: things can be a bit shaky when I accidentally bump against the desk due to the rather extreme lever position I had to chose to make things work in my office, but it has been working just fine now since 2012. Still, at some point I might get something a bit more static. In any case, a monitor mount is something I'd highly recommended for everyone really, even if you don't want if for ergonomic reasons -- I cannot emphasize the increase in desk space enough ;)</p>
<p>
  <img src="./monitors.jpg" alt="My monitors, mounted on a gas lift mount">
</p>
<h2>tl;dr</h2>
<p>Get a split ergo keyboard, a gaming mouse for your primary and a trackball for your off hand. Don't cheap out on your office chair, seriously consider investment in a standing desk, get a balance board with it and finally reclaim your desk and improve your workplace's ergonomy at the same time with a monitor mount.</p>
]]></content:encoded>
            <enclosure url="https://foosel.net/assets/content/blog/2021-03-13-my-workplace/office.jpg" length="0" type="image/jpg"/>
        </item>
        <item>
            <title><![CDATA[Hello World!]]></title>
            <link>https://foosel.net/blog/2021-03-12-hello-world</link>
            <guid>2021-03-12-hello-world</guid>
            <pubDate>Fri, 12 Mar 2021 22:00:00 GMT</pubDate>
            <content:encoded><![CDATA[
<p>
  It's been a couple years since I last tried to maintain a blog. Back then I was still living life as a corporate drone,
  employed as a Software Architect to consult other people on their IT problems. I rarely had anything I could blog about
  -- either things were under NDA, or they were simply uninteresting. Since then my life has been turned completely on its head.
  In late 2012 I got myself a 3d printer, spent my Christmas break to develop a small web interface for it, that grew into a full
  sized Open Source project called <a href="https://octoprint.org">OctoPrint</a> and these days I work full time on it.
</p>
<p>
  I've learned a lot not only on 3d printers &#x26; Python, but also on Open Source development, crowdfunding, the challenges of
  community management, but also on work life balance, workplace ergonomics, home office life and stress management. And that's
  just from my job! In my personal life I've also spent a lot of time tinkering with electronics, learned how to bake bread,
  discovered cooking and went completely down the home automation rabbit hole.
</p>
<p>
  I don't know about you, but I think that should hopefully make for some good opportunities to blog again, and save some of
  my learnings in a more persistent way than the ephemeral nature of twitter threads. So I did what apparently every dev seems
  to do in such a case and spent way too much time on a webpage reboot with some new tooling that I wanted to try anyway,
  and this is the result. This whole page is still a static page, but I've switched it from <a href="https://jekyllrb.com">Jekyll</a>
  to <a href="https://nextjs.org/">next.js</a>. Why? I wanted to get some more hands-on experience with React since I'm evaluating it for a
  new UI for OctoPrint, and I also never really warmed up to Ruby but know JS, so with expandability in mind this just feels like a
  better fit. If you want to study the source, you can find that <a href="https://github.com/foosel/foosel.github.io">here</a> (but please
  don't look too closely, I'm still learning and things are probably not even remotely optimally implemented).
</p>
<p>So here we are, and it's time to fill this up a bit more. Thankfully I already got some ideas...</p>
]]></content:encoded>
            <enclosure url="https://foosel.net/assets/content/blog/2021-03-12-hello-world/new_start.jpg" length="0" type="image/jpg"/>
        </item>
    </channel>
</rss>