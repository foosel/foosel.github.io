{"pageProps":{"post":{"slug":"2021-03-28-homelab-uplink-monitoring","updated":"2021-03-31T12:40:00+0200","title":"Homelab uplink monitoring","subtitle":"Keeping an eye on my ISP's performance","image":{"url":"/assets/content/blog/2021-03-28-homelab-uplink-monitoring/title.png","alt":"My networking dashboard in Grafana with speed and latency monitoring","external":"https://foosel.net/assets/content/blog/2021-03-28-homelab-uplink-monitoring/title.png"},"content":"\nFor a bit more than two years now I've been closely monitoring my network uplink. In the past I had a ton of issues with up- or download speeds not being what I paid for, packet loss issues and outright full blown outages. In order to put myself into a better position when reaching out to the ISP's support hotline I figured it would be good to be able to proof not only the existence of these issues but to also be able to determine the exact times they happened at and also to verify and show that in fact it was only external connections that were suffering and it was not an issue with my own internal network. Given that I don't trust the cable modem/router they force on me to be my edge router and instead have my own Unifi gear set up behind it (considering anything not exclusively under my control to be part of the hostile public internet) this otherwise will usually lead to endless attempts to blame my LAN when in fact the issue lies outside of my reach.\n\nI already had an [InfluxDB](https://www.influxdata.com/) and [Grafana](https://grafana.com/) setup running anyhow for my [Home Assistant instance](https://home-assistant.io/) to dump values from my home climate sensors into, so it was a logical next step to simply add some additional sensors to the mix.\n\n## Throughput\n\nI currently run a speed test of the network throughput every 20min and log the results via MQTT into InfluxDB. I had to find out that neither the speed test integration in Home Assistant nor the official speedtest-cli tool were performing reliably enough for this -- I was constantly getting dips in measured throughput and thus alerts, even when everything was completely fine with my uplink.\n\nI solved this by turning to [speedtest-rs](https://github.com/nelsonjchen/speedtest-rs) and a small shell script that parses the output and pushes it into MQTT to Home Assistant, which then processes it further for some visualization right on my dashboard but also forwards it further into InfluxDB. You can find the `Dockerfile` and the script plus some further info [in this gist](https://gist.github.com/foosel/ef98a5774d1a495ab3781eba8a157fee).\n\nIn Grafana I then use this data to provide me with some single stat panels for the current downstream, upstream and ping values as well as the averages over the selected time range:\n\n![Some single stat panels show current and average down- and upstream speed and measured ping](./currentspeed.png)\n\nAdditionally, I also plot the down- and upstream speed in a timeline, together with the current bandwidth consumption as extracted by Home Assistant from my ISP's cable modem/router (thanks to the [Fritzbox NetMonitor integration](https://www.home-assistant.io/integrations/fritzbox_netmonitor/)). Together, this gives me a good picture of whether there is actually an issue when I see a dip in the measured values, or if it's just too high bandwidth utilization:\n\n![A graph showing measured up- and downstream speed vs consumed up- and downstream bandwidth utilization](./bandwidth.png)\n\nYou can see in these screenshots that I recently upgraded my plan with my ISP -- from 200/20 to 500/50 MBit. The problem: The speedtest run by my monitoring setup doesn't hit the 500 mark, whereas running a manual test on speedtest.net works just fine. Looking at the `speedtest-rs` README it becomes apparent that this is a known issue with the legacy (open) Speedtest.net API:\n\n> This tool currently only supports [HTTP Legacy Fallback](http://www.ookla.com/support/a84541858) for testing.\n>\n> High bandwidth connections higher than ~200Mbps may return incorrect results!\n>\n> The testing operations are different from socket versions of tools connecting to speedtest.net infrastructure. In the many FOSS Go versions, tests are done to find an amount of data that can run for a default of 3 seconds over some TCP connection. In particular, `speedtest-cli` and `speedtest-rs` tests with what Ookla calls the [\"HTTP Legacy Fallback\"](http://www.ookla.com/support/a84541858) for hosts that cannot establish a direct TCP connection.\n\nI fear I might have to look into reimplementing the current speedtest-to-mqtt setup with another container utilizing the official (and sadly proprietary) Speedtest CLI tool to mitigate this issue. Thankfully, it should be quite easy to build a drop-in replacement thanks to the modularization in effect.\n\n_Update from March 30th 2021_ I've now done that and [here's an updated gist](https://gist.github.com/foosel/70ecbeade55cc852dbc0a4f7c4040adc) that works identically to the `speedtest-rs` approach, but instead utilizes [Ookla's official command line tool](https://www.speedtest.net/apps/cli). The results are stable numbers that reflect the expected bandwidth and also match the web based test results.\n\n_Update from March 31st, 2021_ I wasn't too happy with running a proprietary tool for my speed testing, went looking for an OSS alternative, came across [librespeed](https://librespeed.org/) and therefore have now [replicated the setup again using that](https://gist.github.com/foosel/f7d9a08c0445454ab90d6c4974a9e316). You might want to experiment a bit to find a server close to you and define that via `--server <id>`, the auto discovery appears to be a bit wonky. Or just use your own server list via `--server-json` or `--local-json`.\n\n## Latency and packet loss\n\nIn addition to the available up- and downstream speeds, I constantly monitor latency and packet loss to a selected number of hosts both external and internal to my network as well. For this I ping some public DNS servers (Google, Cloudflare and Quadnine) and some of my own vservers for the remote side, and the ISP's Fritzbox, my managed network gear and internal servers for the LAN side. I used to do this via [Smokeping](https://oss.oetiker.ch/smokeping/), but when I set up my InfluxDB/Grafana stack I wanted to find a solution to have everything together in one place.\n\nThankfully I almost immediately found [this post by Tor Hveem](https://hveem.no/visualizing-latency-variance-with-grafana) who solved this with a little custom Go tool to run `fping` against a number of configurable hosts and push the results right into InfluxDB. This was exactly what I wanted and thus I replicated the outlined setup, albeit with a slightly different color scheme.\n\nI use a [modified version of Tor's `infping` tool maintained by Nick Van Wiggeren](https://github.com/nickvanw/infping) and run that in a Docker container on my NAS. You can find everything needed to run this on your own [in this gist](https://gist.github.com/foosel/46804306d510d79f14117f95ed64b877).\n\nAs a result I get ping output for all hosts every 60 sec with times and packet loss information pushed right into InfluxDB. This is easily queried by Grafana and looks quite nice when visualized:\n\n![A graph showing min, avg and max latency and packet loss data for 8.8.8.8](./smokeping.png)\n\nAnd on my network dashboard, I plot only the `avg` values across all hosts and a mean `loss` value into one single graph each for external and internal hosts:\n\n![A graph showing avg latency and packet loss data for all remote hosts](./latency.png)\n\nThis allows me a good overview of the current state of uplink and internal network at one glance.\n\n## Alerts\n\nSince just graphs won't give me an immediate heads-up when something goes wrong, I have a bunch of alerts set up in Grafana:\n\n-   Measured download speed falls beneath 250MBit for more than one hour\n-   Measured upload speed falls beneath 35MBit for more than one hour\n-   Mean packet loss across all external hosts rises above 25% for more than ten minutes\n\nAll of those trigger a notification to a private Discord server (via Grafana's own notification mechanism). In theory this notification should even include a screenshot of the panel for which the alert was triggered for, but I'm having some problems with that still that I need to investigate.\n\n![An example alert and alert clearance notification in Discord](./discord.png)\n\nThis notification channel has an obvious problem: When the uplink goes out completely, I won't get the notification if my phone is in my LAN. I really need to add a local alert as well at some point ðŸ˜…\n\nStill, it usually will give me a heads-up in time for me to reach out to my ISP on short notice and request they start troubleshooting.\n\n## Conclusion\n\nThis monitoring setup has proven valuable in debugging network performance issues and also getting an early heads-up about current ISP issues. I have successfully used screenshots for proving ongoing issues to my ISP, and also sped up the one or other troubleshooting session when there was in fact an issue with my LAN. In my book, that makes it absolutely worth the time it took me to set this up and maintain it. And: it kinda looks cool ðŸ˜Ž\n\nIf you want to give this a go yourself, this might be of interest to you:\n\n-   [Dockerfile, compose and instructions for speedtest container](https://gist.github.com/foosel/f7d9a08c0445454ab90d6c4974a9e316)\n    -   [Ookla speedtest based version](https://gist.github.com/foosel/70ecbeade55cc852dbc0a4f7c4040adc)\n    -   [speedtest-rs based version](https://gist.github.com/foosel/ef98a5774d1a495ab3781eba8a157fee)\n-   [Dockerfile, compose and instructions for infping container](https://gist.github.com/foosel/46804306d510d79f14117f95ed64b877)\n-   [Panel JSON for the mentioned visualizations](https://gist.github.com/foosel/ec0b6355d1d0c3ab65ee4df79d795a73)\n","assets":[{"slug":"2021-03-28-homelab-uplink-monitoring","asset":"bandwidth.png","path":"/home/runner/work/foosel.github.io/foosel.github.io/content/posts/2021-03-28-homelab-uplink-monitoring/bandwidth.png"},{"slug":"2021-03-28-homelab-uplink-monitoring","asset":"currentspeed.png","path":"/home/runner/work/foosel.github.io/foosel.github.io/content/posts/2021-03-28-homelab-uplink-monitoring/currentspeed.png"},{"slug":"2021-03-28-homelab-uplink-monitoring","asset":"discord.png","path":"/home/runner/work/foosel.github.io/foosel.github.io/content/posts/2021-03-28-homelab-uplink-monitoring/discord.png"},{"slug":"2021-03-28-homelab-uplink-monitoring","asset":"latency.png","path":"/home/runner/work/foosel.github.io/foosel.github.io/content/posts/2021-03-28-homelab-uplink-monitoring/latency.png"},{"slug":"2021-03-28-homelab-uplink-monitoring","asset":"smokeping.png","path":"/home/runner/work/foosel.github.io/foosel.github.io/content/posts/2021-03-28-homelab-uplink-monitoring/smokeping.png"},{"slug":"2021-03-28-homelab-uplink-monitoring","asset":"title.png","path":"/home/runner/work/foosel.github.io/foosel.github.io/content/posts/2021-03-28-homelab-uplink-monitoring/title.png"}],"published":"2021-03-28T00:00:00Z","readingtime":{"text":"8 min read","minutes":7.11,"time":426600,"words":1422},"mdx":{"compiledSource":"\"use strict\";\n\nfunction _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar layoutProps = {};\nvar MDXLayout = \"wrapper\";\n\nfunction MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, [\"components\"]);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"p\", null, \"For a bit more than two years now I've been closely monitoring my network uplink. In the past I had a ton of issues with up- or download speeds not being what I paid for, packet loss issues and outright full blown outages. In order to put myself into a better position when reaching out to the ISP's support hotline I figured it would be good to be able to proof not only the existence of these issues but to also be able to determine the exact times they happened at and also to verify and show that in fact it was only external connections that were suffering and it was not an issue with my own internal network. Given that I don't trust the cable modem/router they force on me to be my edge router and instead have my own Unifi gear set up behind it (considering anything not exclusively under my control to be part of the hostile public internet) this otherwise will usually lead to endless attempts to blame my LAN when in fact the issue lies outside of my reach.\"), mdx(\"p\", null, \"I already had an \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://www.influxdata.com/\"\n  }, \"InfluxDB\"), \" and \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://grafana.com/\"\n  }, \"Grafana\"), \" setup running anyhow for my \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://home-assistant.io/\"\n  }, \"Home Assistant instance\"), \" to dump values from my home climate sensors into, so it was a logical next step to simply add some additional sensors to the mix.\"), mdx(\"h2\", null, \"Throughput\"), mdx(\"p\", null, \"I currently run a speed test of the network throughput every 20min and log the results via MQTT into InfluxDB. I had to find out that neither the speed test integration in Home Assistant nor the official speedtest-cli tool were performing reliably enough for this -- I was constantly getting dips in measured throughput and thus alerts, even when everything was completely fine with my uplink.\"), mdx(\"p\", null, \"I solved this by turning to \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://github.com/nelsonjchen/speedtest-rs\"\n  }, \"speedtest-rs\"), \" and a small shell script that parses the output and pushes it into MQTT to Home Assistant, which then processes it further for some visualization right on my dashboard but also forwards it further into InfluxDB. You can find the \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"Dockerfile\"), \" and the script plus some further info \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://gist.github.com/foosel/ef98a5774d1a495ab3781eba8a157fee\"\n  }, \"in this gist\"), \".\"), mdx(\"p\", null, \"In Grafana I then use this data to provide me with some single stat panels for the current downstream, upstream and ping values as well as the averages over the selected time range:\"), mdx(\"p\", null, mdx(\"img\", {\n    parentName: \"p\",\n    \"src\": \"./currentspeed.png\",\n    \"alt\": \"Some single stat panels show current and average down- and upstream speed and measured ping\"\n  })), mdx(\"p\", null, \"Additionally, I also plot the down- and upstream speed in a timeline, together with the current bandwidth consumption as extracted by Home Assistant from my ISP's cable modem/router (thanks to the \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://www.home-assistant.io/integrations/fritzbox_netmonitor/\"\n  }, \"Fritzbox NetMonitor integration\"), \"). Together, this gives me a good picture of whether there is actually an issue when I see a dip in the measured values, or if it's just too high bandwidth utilization:\"), mdx(\"p\", null, mdx(\"img\", {\n    parentName: \"p\",\n    \"src\": \"./bandwidth.png\",\n    \"alt\": \"A graph showing measured up- and downstream speed vs consumed up- and downstream bandwidth utilization\"\n  })), mdx(\"p\", null, \"You can see in these screenshots that I recently upgraded my plan with my ISP -- from 200/20 to 500/50 MBit. The problem: The speedtest run by my monitoring setup doesn't hit the 500 mark, whereas running a manual test on speedtest.net works just fine. Looking at the \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"speedtest-rs\"), \" README it becomes apparent that this is a known issue with the legacy (open) Speedtest.net API:\"), mdx(\"blockquote\", null, mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \"This tool currently only supports \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"http://www.ookla.com/support/a84541858\"\n  }, \"HTTP Legacy Fallback\"), \" for testing.\"), mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \"High bandwidth connections higher than ~200Mbps may return incorrect results!\"), mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \"The testing operations are different from socket versions of tools connecting to speedtest.net infrastructure. In the many FOSS Go versions, tests are done to find an amount of data that can run for a default of 3 seconds over some TCP connection. In particular, \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"speedtest-cli\"), \" and \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"speedtest-rs\"), \" tests with what Ookla calls the \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"http://www.ookla.com/support/a84541858\"\n  }, \"\\\"HTTP Legacy Fallback\\\"\"), \" for hosts that cannot establish a direct TCP connection.\")), mdx(\"p\", null, \"I fear I might have to look into reimplementing the current speedtest-to-mqtt setup with another container utilizing the official (and sadly proprietary) Speedtest CLI tool to mitigate this issue. Thankfully, it should be quite easy to build a drop-in replacement thanks to the modularization in effect.\"), mdx(\"p\", null, mdx(\"em\", {\n    parentName: \"p\"\n  }, \"Update from March 30th 2021\"), \" I've now done that and \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://gist.github.com/foosel/70ecbeade55cc852dbc0a4f7c4040adc\"\n  }, \"here's an updated gist\"), \" that works identically to the \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"speedtest-rs\"), \" approach, but instead utilizes \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://www.speedtest.net/apps/cli\"\n  }, \"Ookla's official command line tool\"), \". The results are stable numbers that reflect the expected bandwidth and also match the web based test results.\"), mdx(\"p\", null, mdx(\"em\", {\n    parentName: \"p\"\n  }, \"Update from March 31st, 2021\"), \" I wasn't too happy with running a proprietary tool for my speed testing, went looking for an OSS alternative, came across \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://librespeed.org/\"\n  }, \"librespeed\"), \" and therefore have now \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://gist.github.com/foosel/f7d9a08c0445454ab90d6c4974a9e316\"\n  }, \"replicated the setup again using that\"), \". You might want to experiment a bit to find a server close to you and define that via \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"--server <id>\"), \", the auto discovery appears to be a bit wonky. Or just use your own server list via \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"--server-json\"), \" or \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"--local-json\"), \".\"), mdx(\"h2\", null, \"Latency and packet loss\"), mdx(\"p\", null, \"In addition to the available up- and downstream speeds, I constantly monitor latency and packet loss to a selected number of hosts both external and internal to my network as well. For this I ping some public DNS servers (Google, Cloudflare and Quadnine) and some of my own vservers for the remote side, and the ISP's Fritzbox, my managed network gear and internal servers for the LAN side. I used to do this via \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://oss.oetiker.ch/smokeping/\"\n  }, \"Smokeping\"), \", but when I set up my InfluxDB/Grafana stack I wanted to find a solution to have everything together in one place.\"), mdx(\"p\", null, \"Thankfully I almost immediately found \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://hveem.no/visualizing-latency-variance-with-grafana\"\n  }, \"this post by Tor Hveem\"), \" who solved this with a little custom Go tool to run \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"fping\"), \" against a number of configurable hosts and push the results right into InfluxDB. This was exactly what I wanted and thus I replicated the outlined setup, albeit with a slightly different color scheme.\"), mdx(\"p\", null, \"I use a \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://github.com/nickvanw/infping\"\n  }, \"modified version of Tor's \", mdx(\"inlineCode\", {\n    parentName: \"a\"\n  }, \"infping\"), \" tool maintained by Nick Van Wiggeren\"), \" and run that in a Docker container on my NAS. You can find everything needed to run this on your own \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://gist.github.com/foosel/46804306d510d79f14117f95ed64b877\"\n  }, \"in this gist\"), \".\"), mdx(\"p\", null, \"As a result I get ping output for all hosts every 60 sec with times and packet loss information pushed right into InfluxDB. This is easily queried by Grafana and looks quite nice when visualized:\"), mdx(\"p\", null, mdx(\"img\", {\n    parentName: \"p\",\n    \"src\": \"./smokeping.png\",\n    \"alt\": \"A graph showing min, avg and max latency and packet loss data for 8.8.8.8\"\n  })), mdx(\"p\", null, \"And on my network dashboard, I plot only the \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"avg\"), \" values across all hosts and a mean \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"loss\"), \" value into one single graph each for external and internal hosts:\"), mdx(\"p\", null, mdx(\"img\", {\n    parentName: \"p\",\n    \"src\": \"./latency.png\",\n    \"alt\": \"A graph showing avg latency and packet loss data for all remote hosts\"\n  })), mdx(\"p\", null, \"This allows me a good overview of the current state of uplink and internal network at one glance.\"), mdx(\"h2\", null, \"Alerts\"), mdx(\"p\", null, \"Since just graphs won't give me an immediate heads-up when something goes wrong, I have a bunch of alerts set up in Grafana:\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Measured download speed falls beneath 250MBit for more than one hour\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Measured upload speed falls beneath 35MBit for more than one hour\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Mean packet loss across all external hosts rises above 25% for more than ten minutes\")), mdx(\"p\", null, \"All of those trigger a notification to a private Discord server (via Grafana's own notification mechanism). In theory this notification should even include a screenshot of the panel for which the alert was triggered for, but I'm having some problems with that still that I need to investigate.\"), mdx(\"p\", null, mdx(\"img\", {\n    parentName: \"p\",\n    \"src\": \"./discord.png\",\n    \"alt\": \"An example alert and alert clearance notification in Discord\"\n  })), mdx(\"p\", null, \"This notification channel has an obvious problem: When the uplink goes out completely, I won't get the notification if my phone is in my LAN. I really need to add a local alert as well at some point \\uD83D\\uDE05\"), mdx(\"p\", null, \"Still, it usually will give me a heads-up in time for me to reach out to my ISP on short notice and request they start troubleshooting.\"), mdx(\"h2\", null, \"Conclusion\"), mdx(\"p\", null, \"This monitoring setup has proven valuable in debugging network performance issues and also getting an early heads-up about current ISP issues. I have successfully used screenshots for proving ongoing issues to my ISP, and also sped up the one or other troubleshooting session when there was in fact an issue with my LAN. In my book, that makes it absolutely worth the time it took me to set this up and maintain it. And: it kinda looks cool \\uD83D\\uDE0E\"), mdx(\"p\", null, \"If you want to give this a go yourself, this might be of interest to you:\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://gist.github.com/foosel/f7d9a08c0445454ab90d6c4974a9e316\"\n  }, \"Dockerfile, compose and instructions for speedtest container\"), mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://gist.github.com/foosel/70ecbeade55cc852dbc0a4f7c4040adc\"\n  }, \"Ookla speedtest based version\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://gist.github.com/foosel/ef98a5774d1a495ab3781eba8a157fee\"\n  }, \"speedtest-rs based version\")))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://gist.github.com/foosel/46804306d510d79f14117f95ed64b877\"\n  }, \"Dockerfile, compose and instructions for infping container\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://gist.github.com/foosel/ec0b6355d1d0c3ab65ee4df79d795a73\"\n  }, \"Panel JSON for the mentioned visualizations\"))));\n}\n\n;\nMDXContent.isMDXComponent = true;","renderedOutput":"<p>For a bit more than two years now I&#x27;ve been closely monitoring my network uplink. In the past I had a ton of issues with up- or download speeds not being what I paid for, packet loss issues and outright full blown outages. In order to put myself into a better position when reaching out to the ISP&#x27;s support hotline I figured it would be good to be able to proof not only the existence of these issues but to also be able to determine the exact times they happened at and also to verify and show that in fact it was only external connections that were suffering and it was not an issue with my own internal network. Given that I don&#x27;t trust the cable modem/router they force on me to be my edge router and instead have my own Unifi gear set up behind it (considering anything not exclusively under my control to be part of the hostile public internet) this otherwise will usually lead to endless attempts to blame my LAN when in fact the issue lies outside of my reach.</p><p>I already had an <a pathname=\"/blog/2021-03-28-homelab-uplink-monitoring\" href=\"https://www.influxdata.com/\">InfluxDB</a> and <a pathname=\"/blog/2021-03-28-homelab-uplink-monitoring\" href=\"https://grafana.com/\">Grafana</a> setup running anyhow for my <a pathname=\"/blog/2021-03-28-homelab-uplink-monitoring\" href=\"https://home-assistant.io/\">Home Assistant instance</a> to dump values from my home climate sensors into, so it was a logical next step to simply add some additional sensors to the mix.</p><h2>Throughput</h2><p>I currently run a speed test of the network throughput every 20min and log the results via MQTT into InfluxDB. I had to find out that neither the speed test integration in Home Assistant nor the official speedtest-cli tool were performing reliably enough for this -- I was constantly getting dips in measured throughput and thus alerts, even when everything was completely fine with my uplink.</p><p>I solved this by turning to <a pathname=\"/blog/2021-03-28-homelab-uplink-monitoring\" href=\"https://github.com/nelsonjchen/speedtest-rs\">speedtest-rs</a> and a small shell script that parses the output and pushes it into MQTT to Home Assistant, which then processes it further for some visualization right on my dashboard but also forwards it further into InfluxDB. You can find the <code>Dockerfile</code> and the script plus some further info <a pathname=\"/blog/2021-03-28-homelab-uplink-monitoring\" href=\"https://gist.github.com/foosel/ef98a5774d1a495ab3781eba8a157fee\">in this gist</a>.</p><p>In Grafana I then use this data to provide me with some single stat panels for the current downstream, upstream and ping values as well as the averages over the selected time range:</p><p><img pathname=\"/blog/2021-03-28-homelab-uplink-monitoring\" src=\"/assets/content/blog/2021-03-28-homelab-uplink-monitoring/currentspeed.png\" alt=\"Some single stat panels show current and average down- and upstream speed and measured ping\"/></p><p>Additionally, I also plot the down- and upstream speed in a timeline, together with the current bandwidth consumption as extracted by Home Assistant from my ISP&#x27;s cable modem/router (thanks to the <a pathname=\"/blog/2021-03-28-homelab-uplink-monitoring\" href=\"https://www.home-assistant.io/integrations/fritzbox_netmonitor/\">Fritzbox NetMonitor integration</a>). Together, this gives me a good picture of whether there is actually an issue when I see a dip in the measured values, or if it&#x27;s just too high bandwidth utilization:</p><p><img pathname=\"/blog/2021-03-28-homelab-uplink-monitoring\" src=\"/assets/content/blog/2021-03-28-homelab-uplink-monitoring/bandwidth.png\" alt=\"A graph showing measured up- and downstream speed vs consumed up- and downstream bandwidth utilization\"/></p><p>You can see in these screenshots that I recently upgraded my plan with my ISP -- from 200/20 to 500/50 MBit. The problem: The speedtest run by my monitoring setup doesn&#x27;t hit the 500 mark, whereas running a manual test on speedtest.net works just fine. Looking at the <code>speedtest-rs</code> README it becomes apparent that this is a known issue with the legacy (open) Speedtest.net API:</p><blockquote><p>This tool currently only supports <a pathname=\"/blog/2021-03-28-homelab-uplink-monitoring\" href=\"http://www.ookla.com/support/a84541858\">HTTP Legacy Fallback</a> for testing.</p><p>High bandwidth connections higher than ~200Mbps may return incorrect results!</p><p>The testing operations are different from socket versions of tools connecting to speedtest.net infrastructure. In the many FOSS Go versions, tests are done to find an amount of data that can run for a default of 3 seconds over some TCP connection. In particular, <code>speedtest-cli</code> and <code>speedtest-rs</code> tests with what Ookla calls the <a pathname=\"/blog/2021-03-28-homelab-uplink-monitoring\" href=\"http://www.ookla.com/support/a84541858\">&quot;HTTP Legacy Fallback&quot;</a> for hosts that cannot establish a direct TCP connection.</p></blockquote><p>I fear I might have to look into reimplementing the current speedtest-to-mqtt setup with another container utilizing the official (and sadly proprietary) Speedtest CLI tool to mitigate this issue. Thankfully, it should be quite easy to build a drop-in replacement thanks to the modularization in effect.</p><p><em>Update from March 30th 2021</em> I&#x27;ve now done that and <a pathname=\"/blog/2021-03-28-homelab-uplink-monitoring\" href=\"https://gist.github.com/foosel/70ecbeade55cc852dbc0a4f7c4040adc\">here&#x27;s an updated gist</a> that works identically to the <code>speedtest-rs</code> approach, but instead utilizes <a pathname=\"/blog/2021-03-28-homelab-uplink-monitoring\" href=\"https://www.speedtest.net/apps/cli\">Ookla&#x27;s official command line tool</a>. The results are stable numbers that reflect the expected bandwidth and also match the web based test results.</p><p><em>Update from March 31st, 2021</em> I wasn&#x27;t too happy with running a proprietary tool for my speed testing, went looking for an OSS alternative, came across <a pathname=\"/blog/2021-03-28-homelab-uplink-monitoring\" href=\"https://librespeed.org/\">librespeed</a> and therefore have now <a pathname=\"/blog/2021-03-28-homelab-uplink-monitoring\" href=\"https://gist.github.com/foosel/f7d9a08c0445454ab90d6c4974a9e316\">replicated the setup again using that</a>. You might want to experiment a bit to find a server close to you and define that via <code>--server &lt;id&gt;</code>, the auto discovery appears to be a bit wonky. Or just use your own server list via <code>--server-json</code> or <code>--local-json</code>.</p><h2>Latency and packet loss</h2><p>In addition to the available up- and downstream speeds, I constantly monitor latency and packet loss to a selected number of hosts both external and internal to my network as well. For this I ping some public DNS servers (Google, Cloudflare and Quadnine) and some of my own vservers for the remote side, and the ISP&#x27;s Fritzbox, my managed network gear and internal servers for the LAN side. I used to do this via <a pathname=\"/blog/2021-03-28-homelab-uplink-monitoring\" href=\"https://oss.oetiker.ch/smokeping/\">Smokeping</a>, but when I set up my InfluxDB/Grafana stack I wanted to find a solution to have everything together in one place.</p><p>Thankfully I almost immediately found <a pathname=\"/blog/2021-03-28-homelab-uplink-monitoring\" href=\"https://hveem.no/visualizing-latency-variance-with-grafana\">this post by Tor Hveem</a> who solved this with a little custom Go tool to run <code>fping</code> against a number of configurable hosts and push the results right into InfluxDB. This was exactly what I wanted and thus I replicated the outlined setup, albeit with a slightly different color scheme.</p><p>I use a <a pathname=\"/blog/2021-03-28-homelab-uplink-monitoring\" href=\"https://github.com/nickvanw/infping\">modified version of Tor&#x27;s <code>infping</code> tool maintained by Nick Van Wiggeren</a> and run that in a Docker container on my NAS. You can find everything needed to run this on your own <a pathname=\"/blog/2021-03-28-homelab-uplink-monitoring\" href=\"https://gist.github.com/foosel/46804306d510d79f14117f95ed64b877\">in this gist</a>.</p><p>As a result I get ping output for all hosts every 60 sec with times and packet loss information pushed right into InfluxDB. This is easily queried by Grafana and looks quite nice when visualized:</p><p><img pathname=\"/blog/2021-03-28-homelab-uplink-monitoring\" src=\"/assets/content/blog/2021-03-28-homelab-uplink-monitoring/smokeping.png\" alt=\"A graph showing min, avg and max latency and packet loss data for 8.8.8.8\"/></p><p>And on my network dashboard, I plot only the <code>avg</code> values across all hosts and a mean <code>loss</code> value into one single graph each for external and internal hosts:</p><p><img pathname=\"/blog/2021-03-28-homelab-uplink-monitoring\" src=\"/assets/content/blog/2021-03-28-homelab-uplink-monitoring/latency.png\" alt=\"A graph showing avg latency and packet loss data for all remote hosts\"/></p><p>This allows me a good overview of the current state of uplink and internal network at one glance.</p><h2>Alerts</h2><p>Since just graphs won&#x27;t give me an immediate heads-up when something goes wrong, I have a bunch of alerts set up in Grafana:</p><ul><li>Measured download speed falls beneath 250MBit for more than one hour</li><li>Measured upload speed falls beneath 35MBit for more than one hour</li><li>Mean packet loss across all external hosts rises above 25% for more than ten minutes</li></ul><p>All of those trigger a notification to a private Discord server (via Grafana&#x27;s own notification mechanism). In theory this notification should even include a screenshot of the panel for which the alert was triggered for, but I&#x27;m having some problems with that still that I need to investigate.</p><p><img pathname=\"/blog/2021-03-28-homelab-uplink-monitoring\" src=\"/assets/content/blog/2021-03-28-homelab-uplink-monitoring/discord.png\" alt=\"An example alert and alert clearance notification in Discord\"/></p><p>This notification channel has an obvious problem: When the uplink goes out completely, I won&#x27;t get the notification if my phone is in my LAN. I really need to add a local alert as well at some point ðŸ˜…</p><p>Still, it usually will give me a heads-up in time for me to reach out to my ISP on short notice and request they start troubleshooting.</p><h2>Conclusion</h2><p>This monitoring setup has proven valuable in debugging network performance issues and also getting an early heads-up about current ISP issues. I have successfully used screenshots for proving ongoing issues to my ISP, and also sped up the one or other troubleshooting session when there was in fact an issue with my LAN. In my book, that makes it absolutely worth the time it took me to set this up and maintain it. And: it kinda looks cool ðŸ˜Ž</p><p>If you want to give this a go yourself, this might be of interest to you:</p><ul><li><a pathname=\"/blog/2021-03-28-homelab-uplink-monitoring\" href=\"https://gist.github.com/foosel/f7d9a08c0445454ab90d6c4974a9e316\">Dockerfile, compose and instructions for speedtest container</a><ul><li><a pathname=\"/blog/2021-03-28-homelab-uplink-monitoring\" href=\"https://gist.github.com/foosel/70ecbeade55cc852dbc0a4f7c4040adc\">Ookla speedtest based version</a></li><li><a pathname=\"/blog/2021-03-28-homelab-uplink-monitoring\" href=\"https://gist.github.com/foosel/ef98a5774d1a495ab3781eba8a157fee\">speedtest-rs based version</a></li></ul></li><li><a pathname=\"/blog/2021-03-28-homelab-uplink-monitoring\" href=\"https://gist.github.com/foosel/46804306d510d79f14117f95ed64b877\">Dockerfile, compose and instructions for infping container</a></li><li><a pathname=\"/blog/2021-03-28-homelab-uplink-monitoring\" href=\"https://gist.github.com/foosel/ec0b6355d1d0c3ab65ee4df79d795a73\">Panel JSON for the mentioned visualizations</a></li></ul>","scope":{}}},"seo":{"title":"Homelab uplink monitoring","description":"Keeping an eye on my ISP's performance","openGraph":{"title":"Homelab uplink monitoring","url":"https://foosel.net/blog/2021-03-28-homelab-uplink-monitoring","description":"Keeping an eye on my ISP's performance","images":[{"url":"https://foosel.net/assets/content/blog/2021-03-28-homelab-uplink-monitoring/title.png","alt":"My networking dashboard in Grafana with speed and latency monitoring","external":"https://foosel.net/assets/content/blog/2021-03-28-homelab-uplink-monitoring/title.png"}],"type":"article","article":{"publishedTime":"2021-03-28T00:00:00Z","modifiedTime":"2021-03-31T12:40:00+0200"}}},"previous":{"link":"/blog/2021-03-19-on-wrong-assumptions","title":"On wrong assumptions"},"next":null},"__N_SSG":true}