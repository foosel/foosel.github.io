{"pageProps":{"post":{"slug":"2021-03-19-on-wrong-assumptions","title":"On wrong assumptions","subtitle":"How I once spent two weeks barking up the wrong tree","image":{"url":"/assets/content/blog/2021-03-19-on-wrong-assumptions/screenshot.jpg","alt":"A shot of the screen displaying the diff of the fix","external":"https://foosel.net/assets/content/blog/2021-03-19-on-wrong-assumptions/screenshot.jpg"},"content":"\n*The original version of this post was published as a [Twitter thread on March 23rd 2020](https://twitter.com/foosel/status/1242121324438355974). I figured I should give it a more permanent home here since IMHO it was a quite fun story.*\n\nSince everyone can use some entertainment right now, how about a battle story on how a year ago I spent almost two weeks trying to wrap my head around a really weird issue of a lagging GCODE viewer and overall print progress reporting in [OctoPrint](https://octoprint.org) and finally figuring it out?\n\nOur story begins around the release of 1.4.0, when [a new topic on the community forum](https://community.octoprint.org/t/curious-issue-with-print-progress/16304) showed up:\n\n> ### Curious issue with print progress \n>\n> The print progress figures on my Octopi setup are lagging behind the actual print. [...] Nothing is broken - anything I throw at it (an Ender 3) prints fine but as a print progresses, the percentage complete, current layer, and sync'd gcode viewer gradually lag behind what is actually being printed. For example, on a print with 400 layers, as the last layer is printed the reported progress and current layer is around 96% and 385 respectively. If I do a quick calculation of the displayed Printed/Total file size figures it works out at 96% but what it has actually printed is over 99%. When the print finishes the numbers jump to 100% and 400 and everything is fine.\n>\n> [...]\n\nThis was indeed a very curious issue, since due to the nature of the communication with the printer and buffering in the firmware the progress is usually rather slightly *ahead* than behind. Some quick testing on my end showed no reproduction, however more and more people chimed in with the same observation. \n\nI was stumped.\n\nMy first approach was to collect information from those affected by it. Printer model, firmware version, installed plugins, used slicer and so on. It soon turned out that all affected installations were using Ultimaker Cura as the slicer.\n\nA quick test by the OP with a different slicer confirmed that it indeed just occurred with GCODE sliced by Cura for him, same file in another slicer had everything work as designed. However, comparing the GCODE revealed no immediate differences that would explain this, and what actually is *in* the file also doesn't really play into progress tracking. My own experiments with Cura failed to reproduce.\n\nConvinced that the issue must be some sort of delay between the backend and the frontend -- maybe due to network issues? -- I whipped up a plugin (since deleted) to log progress on both ends to a log which could then be shared and analysed. The first results came in an guess what? I had barked up the wrong tree, the reported progress was identical. So back to square one.\n\nI still couldn't reproduce it on my end and was starting to get really angry at this issue ðŸ˜… I finally threw a copy of some GCODE files now shared by the reporter of the issue on my own printer and *finally* I could reproduce. Which doesn't mean I had any idea WTF was going on though.\n\nAfter many test prints, head scratching and going through the files with a comb I finally noticed something. The files with the issue had `CRLF` (or `\\r\\n`) line endings. Those without (including my own sliced files) had just `LF` (or `\\n`) line endings.\n\nSo that made me go ðŸ¤¨ Some cursing and breakpoint setting later I had proof that the reported progress in backend and frontend was flawed to begin with. I could see that a line was being reported with a file position that it actually was not located at in the file, and which instead belonged to a couple lines earlier. Which meant my positions were reported wrong right at the source -- with a lag. And then it suddenly hit me.\n\nBut before I can tell you what was happening I need to give you some background on how OctoPrint reads GCODE files it's printing in order to understand what was going on. Printed files are read line by line because that is how they are sent to the printer. For that OctoPrint uses the [`readline`](https://docs.python.org/3/library/io.html?highlight=readline#io.IOBase.readline) method of the file stream. And that works by reading chunks of data from the file until a line separator is found, returning everything read up to this separator and saving the rest for the next line to be read. That means the file will have to be read further than what is returned. And that means that the position in the open file as reported by [`tell`](https://docs.python.org/3/library/io.html?highlight=readline#io.IOBase.tell) on the file stream will always be slightly ahead. For progress reporting in OctoPrint however I need to know the exact byte position of each line in the file. So what I do instead of relying on the internal and slightly ahead file position is that I increase my own position indicator by the length of the line read from the file. And this is where my problem was located. \n\nIt turns out that for some reason I wasn't getting the lines back from `readline` with the original line endings attached. Instead I always got `LF`, even for files with `CRLF`. And that means I was counting one byte short for every single line in `CRLF` terminated files. One byte short per line doesn't sound like much, but that adds up through a file with several hundred thousands of lines, to a point where progress reporting will be off by whole layers the further in the print and thus the file you are.\n\nBut what was the reason for this popping up in 1.4.0? I hadn't modified the code in question at all. It had been the same since 2016 actually. Well, it turns out that a tiny change during the Python 3 compatibility migration done to a helper function I used in that code had interesting side effects: switching from [`codecs.open`](https://docs.python.org/3/library/codecs.html#codecs.open) to [`io.open`](https://docs.python.org/3/library/io.html#io.open). \n\nIt turns out that `io.open` (and thus Python 3's built-in `open`) by default will open text files in \"universal newlines mode\" (see [PEP278](https://www.python.org/dev/peps/pep-0278/)), meaning it will happily parse every common line ending, but convert it to `LF` before returning. Which caused my off-by-one issue in files with `CRLF`.\n\nAnd the fix? [Setting `newline=\"\"` on the open call](https://github.com/foosel/OctoPrint/commit/27bbab9582eb3a1a9fca8f2b203e88b1682fcdc5): \n\n``` diff\ndiff --git a/src/octoprint/util/comm.py b/src/octoprint/util/comm.py\nindex 67191a7af..a6dfc1e24 100644\n--- a/src/octoprint/util/comm.py\n+++ b/src/octoprint/util/comm.py\n@@ -4078,7 +4078,7 @@ def start(self):\n \t\t\"\"\"\n \t\tPrintingFileInformation.start(self)\n \t\twith self._handle_mutex:\n-\t\t\tself._handle = bom_aware_open(self._filename, encoding=\"utf-8\", errors=\"replace\")\n+\t\t\tself._handle = bom_aware_open(self._filename, encoding=\"utf-8\", errors=\"replace\", newline=\"\")\n \t\t\tself._pos = self._handle.tell()\n \t\t\tif self._handle.encoding.endswith(\"-sig\"):\n \t\t\t\t# Apparently we found an utf-8 bom in the file.\n```\n\nThe moral of the story? Don't trust your file position calculations. I could have saved myself a lot of time on debugging this if I had just looked there *first* instead of assuming this code to be fine ðŸ˜…\n\nIn the end, even a year later, I still have no idea why Cura produced `CRLF` code for some and `LF` for me, but I also never really looked hard. A UNIX vs Windows issue can be ruled out here since the affected parties and me were all using Windows. It made me learn something about `io.open` and was a valuable lesson on wrong assumptions however!","assets":[{"slug":"2021-03-19-on-wrong-assumptions","asset":"screenshot.jpg","path":"/home/runner/work/foosel.github.io/foosel.github.io/content/posts/2021-03-19-on-wrong-assumptions/screenshot.jpg"}],"published":"2021-03-19T00:00:00Z","readingtime":{"text":"7 min read","minutes":6.035,"time":362100,"words":1207},"mdx":{"compiledSource":"\"use strict\";\n\nfunction _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar layoutProps = {};\nvar MDXLayout = \"wrapper\";\n\nfunction MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, [\"components\"]);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"p\", null, mdx(\"em\", {\n    parentName: \"p\"\n  }, \"The original version of this post was published as a \", mdx(\"a\", {\n    parentName: \"em\",\n    \"href\": \"https://twitter.com/foosel/status/1242121324438355974\"\n  }, \"Twitter thread on March 23rd 2020\"), \". I figured I should give it a more permanent home here since IMHO it was a quite fun story.\")), mdx(\"p\", null, \"Since everyone can use some entertainment right now, how about a battle story on how a year ago I spent almost two weeks trying to wrap my head around a really weird issue of a lagging GCODE viewer and overall print progress reporting in \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://octoprint.org\"\n  }, \"OctoPrint\"), \" and finally figuring it out?\"), mdx(\"p\", null, \"Our story begins around the release of 1.4.0, when \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://community.octoprint.org/t/curious-issue-with-print-progress/16304\"\n  }, \"a new topic on the community forum\"), \" showed up:\"), mdx(\"blockquote\", null, mdx(\"h3\", {\n    parentName: \"blockquote\"\n  }, \"Curious issue with print progress\"), mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \"The print progress figures on my Octopi setup are lagging behind the actual print. \", \"[...]\", \" Nothing is broken - anything I throw at it (an Ender 3) prints fine but as a print progresses, the percentage complete, current layer, and sync'd gcode viewer gradually lag behind what is actually being printed. For example, on a print with 400 layers, as the last layer is printed the reported progress and current layer is around 96% and 385 respectively. If I do a quick calculation of the displayed Printed/Total file size figures it works out at 96% but what it has actually printed is over 99%. When the print finishes the numbers jump to 100% and 400 and everything is fine.\"), mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \"[...]\")), mdx(\"p\", null, \"This was indeed a very curious issue, since due to the nature of the communication with the printer and buffering in the firmware the progress is usually rather slightly \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"ahead\"), \" than behind. Some quick testing on my end showed no reproduction, however more and more people chimed in with the same observation. \"), mdx(\"p\", null, \"I was stumped.\"), mdx(\"p\", null, \"My first approach was to collect information from those affected by it. Printer model, firmware version, installed plugins, used slicer and so on. It soon turned out that all affected installations were using Ultimaker Cura as the slicer.\"), mdx(\"p\", null, \"A quick test by the OP with a different slicer confirmed that it indeed just occurred with GCODE sliced by Cura for him, same file in another slicer had everything work as designed. However, comparing the GCODE revealed no immediate differences that would explain this, and what actually is \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"in\"), \" the file also doesn't really play into progress tracking. My own experiments with Cura failed to reproduce.\"), mdx(\"p\", null, \"Convinced that the issue must be some sort of delay between the backend and the frontend -- maybe due to network issues? -- I whipped up a plugin (since deleted) to log progress on both ends to a log which could then be shared and analysed. The first results came in an guess what? I had barked up the wrong tree, the reported progress was identical. So back to square one.\"), mdx(\"p\", null, \"I still couldn't reproduce it on my end and was starting to get really angry at this issue \\uD83D\\uDE05 I finally threw a copy of some GCODE files now shared by the reporter of the issue on my own printer and \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"finally\"), \" I could reproduce. Which doesn't mean I had any idea WTF was going on though.\"), mdx(\"p\", null, \"After many test prints, head scratching and going through the files with a comb I finally noticed something. The files with the issue had \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"CRLF\"), \" (or \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"\\\\r\\\\n\"), \") line endings. Those without (including my own sliced files) had just \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"LF\"), \" (or \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"\\\\n\"), \") line endings.\"), mdx(\"p\", null, \"So that made me go \\uD83E\\uDD28 Some cursing and breakpoint setting later I had proof that the reported progress in backend and frontend was flawed to begin with. I could see that a line was being reported with a file position that it actually was not located at in the file, and which instead belonged to a couple lines earlier. Which meant my positions were reported wrong right at the source -- with a lag. And then it suddenly hit me.\"), mdx(\"p\", null, \"But before I can tell you what was happening I need to give you some background on how OctoPrint reads GCODE files it's printing in order to understand what was going on. Printed files are read line by line because that is how they are sent to the printer. For that OctoPrint uses the \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://docs.python.org/3/library/io.html?highlight=readline#io.IOBase.readline\"\n  }, mdx(\"inlineCode\", {\n    parentName: \"a\"\n  }, \"readline\")), \" method of the file stream. And that works by reading chunks of data from the file until a line separator is found, returning everything read up to this separator and saving the rest for the next line to be read. That means the file will have to be read further than what is returned. And that means that the position in the open file as reported by \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://docs.python.org/3/library/io.html?highlight=readline#io.IOBase.tell\"\n  }, mdx(\"inlineCode\", {\n    parentName: \"a\"\n  }, \"tell\")), \" on the file stream will always be slightly ahead. For progress reporting in OctoPrint however I need to know the exact byte position of each line in the file. So what I do instead of relying on the internal and slightly ahead file position is that I increase my own position indicator by the length of the line read from the file. And this is where my problem was located. \"), mdx(\"p\", null, \"It turns out that for some reason I wasn't getting the lines back from \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"readline\"), \" with the original line endings attached. Instead I always got \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"LF\"), \", even for files with \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"CRLF\"), \". And that means I was counting one byte short for every single line in \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"CRLF\"), \" terminated files. One byte short per line doesn't sound like much, but that adds up through a file with several hundred thousands of lines, to a point where progress reporting will be off by whole layers the further in the print and thus the file you are.\"), mdx(\"p\", null, \"But what was the reason for this popping up in 1.4.0? I hadn't modified the code in question at all. It had been the same since 2016 actually. Well, it turns out that a tiny change during the Python 3 compatibility migration done to a helper function I used in that code had interesting side effects: switching from \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://docs.python.org/3/library/codecs.html#codecs.open\"\n  }, mdx(\"inlineCode\", {\n    parentName: \"a\"\n  }, \"codecs.open\")), \" to \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://docs.python.org/3/library/io.html#io.open\"\n  }, mdx(\"inlineCode\", {\n    parentName: \"a\"\n  }, \"io.open\")), \". \"), mdx(\"p\", null, \"It turns out that \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"io.open\"), \" (and thus Python 3's built-in \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"open\"), \") by default will open text files in \\\"universal newlines mode\\\" (see \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://www.python.org/dev/peps/pep-0278/\"\n  }, \"PEP278\"), \"), meaning it will happily parse every common line ending, but convert it to \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"LF\"), \" before returning. Which caused my off-by-one issue in files with \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"CRLF\"), \".\"), mdx(\"p\", null, \"And the fix? \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://github.com/foosel/OctoPrint/commit/27bbab9582eb3a1a9fca8f2b203e88b1682fcdc5\"\n  }, \"Setting \", mdx(\"inlineCode\", {\n    parentName: \"a\"\n  }, \"newline=\\\"\\\"\"), \" on the open call\"), \": \"), mdx(\"pre\", {\n    \"className\": \"language-diff\"\n  }, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-diff\"\n  }, \"diff --git a/src/octoprint/util/comm.py b/src/octoprint/util/comm.py\\nindex 67191a7af..a6dfc1e24 100644\\n--- a/src/octoprint/util/comm.py\\n+++ b/src/octoprint/util/comm.py\\n@@ -4078,7 +4078,7 @@ def start(self):\\n        \\\"\\\"\\\"\\n        PrintingFileInformation.start(self)\\n        with self._handle_mutex:\\n-           self._handle = bom_aware_open(self._filename, encoding=\\\"utf-8\\\", errors=\\\"replace\\\")\\n+           self._handle = bom_aware_open(self._filename, encoding=\\\"utf-8\\\", errors=\\\"replace\\\", newline=\\\"\\\")\\n            self._pos = self._handle.tell()\\n            if self._handle.encoding.endswith(\\\"-sig\\\"):\\n                # Apparently we found an utf-8 bom in the file.\\n\")), mdx(\"p\", null, \"The moral of the story? Don't trust your file position calculations. I could have saved myself a lot of time on debugging this if I had just looked there \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"first\"), \" instead of assuming this code to be fine \\uD83D\\uDE05\"), mdx(\"p\", null, \"In the end, even a year later, I still have no idea why Cura produced \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"CRLF\"), \" code for some and \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"LF\"), \" for me, but I also never really looked hard. A UNIX vs Windows issue can be ruled out here since the affected parties and me were all using Windows. It made me learn something about \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"io.open\"), \" and was a valuable lesson on wrong assumptions however!\"));\n}\n\n;\nMDXContent.isMDXComponent = true;","renderedOutput":"<p><em>The original version of this post was published as a <a pathname=\"/blog/2021-03-19-on-wrong-assumptions\" href=\"https://twitter.com/foosel/status/1242121324438355974\">Twitter thread on March 23rd 2020</a>. I figured I should give it a more permanent home here since IMHO it was a quite fun story.</em></p><p>Since everyone can use some entertainment right now, how about a battle story on how a year ago I spent almost two weeks trying to wrap my head around a really weird issue of a lagging GCODE viewer and overall print progress reporting in <a pathname=\"/blog/2021-03-19-on-wrong-assumptions\" href=\"https://octoprint.org\">OctoPrint</a> and finally figuring it out?</p><p>Our story begins around the release of 1.4.0, when <a pathname=\"/blog/2021-03-19-on-wrong-assumptions\" href=\"https://community.octoprint.org/t/curious-issue-with-print-progress/16304\">a new topic on the community forum</a> showed up:</p><blockquote><h3>Curious issue with print progress</h3><p>The print progress figures on my Octopi setup are lagging behind the actual print. <!-- -->[...]<!-- --> Nothing is broken - anything I throw at it (an Ender 3) prints fine but as a print progresses, the percentage complete, current layer, and sync&#x27;d gcode viewer gradually lag behind what is actually being printed. For example, on a print with 400 layers, as the last layer is printed the reported progress and current layer is around 96% and 385 respectively. If I do a quick calculation of the displayed Printed/Total file size figures it works out at 96% but what it has actually printed is over 99%. When the print finishes the numbers jump to 100% and 400 and everything is fine.</p><p>[...]</p></blockquote><p>This was indeed a very curious issue, since due to the nature of the communication with the printer and buffering in the firmware the progress is usually rather slightly <em>ahead</em> than behind. Some quick testing on my end showed no reproduction, however more and more people chimed in with the same observation. </p><p>I was stumped.</p><p>My first approach was to collect information from those affected by it. Printer model, firmware version, installed plugins, used slicer and so on. It soon turned out that all affected installations were using Ultimaker Cura as the slicer.</p><p>A quick test by the OP with a different slicer confirmed that it indeed just occurred with GCODE sliced by Cura for him, same file in another slicer had everything work as designed. However, comparing the GCODE revealed no immediate differences that would explain this, and what actually is <em>in</em> the file also doesn&#x27;t really play into progress tracking. My own experiments with Cura failed to reproduce.</p><p>Convinced that the issue must be some sort of delay between the backend and the frontend -- maybe due to network issues? -- I whipped up a plugin (since deleted) to log progress on both ends to a log which could then be shared and analysed. The first results came in an guess what? I had barked up the wrong tree, the reported progress was identical. So back to square one.</p><p>I still couldn&#x27;t reproduce it on my end and was starting to get really angry at this issue ðŸ˜… I finally threw a copy of some GCODE files now shared by the reporter of the issue on my own printer and <em>finally</em> I could reproduce. Which doesn&#x27;t mean I had any idea WTF was going on though.</p><p>After many test prints, head scratching and going through the files with a comb I finally noticed something. The files with the issue had <code>CRLF</code> (or <code>\\r\\n</code>) line endings. Those without (including my own sliced files) had just <code>LF</code> (or <code>\\n</code>) line endings.</p><p>So that made me go ðŸ¤¨ Some cursing and breakpoint setting later I had proof that the reported progress in backend and frontend was flawed to begin with. I could see that a line was being reported with a file position that it actually was not located at in the file, and which instead belonged to a couple lines earlier. Which meant my positions were reported wrong right at the source -- with a lag. And then it suddenly hit me.</p><p>But before I can tell you what was happening I need to give you some background on how OctoPrint reads GCODE files it&#x27;s printing in order to understand what was going on. Printed files are read line by line because that is how they are sent to the printer. For that OctoPrint uses the <a pathname=\"/blog/2021-03-19-on-wrong-assumptions\" href=\"https://docs.python.org/3/library/io.html?highlight=readline#io.IOBase.readline\"><code>readline</code></a> method of the file stream. And that works by reading chunks of data from the file until a line separator is found, returning everything read up to this separator and saving the rest for the next line to be read. That means the file will have to be read further than what is returned. And that means that the position in the open file as reported by <a pathname=\"/blog/2021-03-19-on-wrong-assumptions\" href=\"https://docs.python.org/3/library/io.html?highlight=readline#io.IOBase.tell\"><code>tell</code></a> on the file stream will always be slightly ahead. For progress reporting in OctoPrint however I need to know the exact byte position of each line in the file. So what I do instead of relying on the internal and slightly ahead file position is that I increase my own position indicator by the length of the line read from the file. And this is where my problem was located. </p><p>It turns out that for some reason I wasn&#x27;t getting the lines back from <code>readline</code> with the original line endings attached. Instead I always got <code>LF</code>, even for files with <code>CRLF</code>. And that means I was counting one byte short for every single line in <code>CRLF</code> terminated files. One byte short per line doesn&#x27;t sound like much, but that adds up through a file with several hundred thousands of lines, to a point where progress reporting will be off by whole layers the further in the print and thus the file you are.</p><p>But what was the reason for this popping up in 1.4.0? I hadn&#x27;t modified the code in question at all. It had been the same since 2016 actually. Well, it turns out that a tiny change during the Python 3 compatibility migration done to a helper function I used in that code had interesting side effects: switching from <a pathname=\"/blog/2021-03-19-on-wrong-assumptions\" href=\"https://docs.python.org/3/library/codecs.html#codecs.open\"><code>codecs.open</code></a> to <a pathname=\"/blog/2021-03-19-on-wrong-assumptions\" href=\"https://docs.python.org/3/library/io.html#io.open\"><code>io.open</code></a>. </p><p>It turns out that <code>io.open</code> (and thus Python 3&#x27;s built-in <code>open</code>) by default will open text files in &quot;universal newlines mode&quot; (see <a pathname=\"/blog/2021-03-19-on-wrong-assumptions\" href=\"https://www.python.org/dev/peps/pep-0278/\">PEP278</a>), meaning it will happily parse every common line ending, but convert it to <code>LF</code> before returning. Which caused my off-by-one issue in files with <code>CRLF</code>.</p><p>And the fix? <a pathname=\"/blog/2021-03-19-on-wrong-assumptions\" href=\"https://github.com/foosel/OctoPrint/commit/27bbab9582eb3a1a9fca8f2b203e88b1682fcdc5\">Setting <code>newline=&quot;&quot;</code> on the open call</a>: </p><pre class=\"language-diff\"><code class=\"language-diff\">diff --git a/src/octoprint/util/comm.py b/src/octoprint/util/comm.py\nindex 67191a7af..a6dfc1e24 100644\n--- a/src/octoprint/util/comm.py\n+++ b/src/octoprint/util/comm.py\n@@ -4078,7 +4078,7 @@ def start(self):\n        &quot;&quot;&quot;\n        PrintingFileInformation.start(self)\n        with self._handle_mutex:\n-           self._handle = bom_aware_open(self._filename, encoding=&quot;utf-8&quot;, errors=&quot;replace&quot;)\n+           self._handle = bom_aware_open(self._filename, encoding=&quot;utf-8&quot;, errors=&quot;replace&quot;, newline=&quot;&quot;)\n            self._pos = self._handle.tell()\n            if self._handle.encoding.endswith(&quot;-sig&quot;):\n                # Apparently we found an utf-8 bom in the file.\n</code></pre><p>The moral of the story? Don&#x27;t trust your file position calculations. I could have saved myself a lot of time on debugging this if I had just looked there <em>first</em> instead of assuming this code to be fine ðŸ˜…</p><p>In the end, even a year later, I still have no idea why Cura produced <code>CRLF</code> code for some and <code>LF</code> for me, but I also never really looked hard. A UNIX vs Windows issue can be ruled out here since the affected parties and me were all using Windows. It made me learn something about <code>io.open</code> and was a valuable lesson on wrong assumptions however!</p>","scope":{}}},"seo":{"title":"On wrong assumptions","description":"How I once spent two weeks barking up the wrong tree","openGraph":{"title":"On wrong assumptions","url":"https://foosel.net/blog/2021-03-19-on-wrong-assumptions","description":"How I once spent two weeks barking up the wrong tree","images":[{"url":"https://foosel.net/assets/content/blog/2021-03-19-on-wrong-assumptions/screenshot.jpg","alt":"A shot of the screen displaying the diff of the fix","external":"https://foosel.net/assets/content/blog/2021-03-19-on-wrong-assumptions/screenshot.jpg"}],"type":"article","article":{"publishedTime":"2021-03-19T00:00:00Z","modifiedTime":"2021-03-19T00:00:00Z"}}},"previous":{"link":"/blog/2021-03-13-my-workplace","title":"My workplace setup"},"next":{"link":"/blog/2021-03-28-homelab-uplink-monitoring","title":"Homelab uplink monitoring"}},"__N_SSG":true}